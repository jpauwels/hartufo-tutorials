{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hartufo import CipicPlane\n",
    "from hartufo import ImageSpec, SubjectSpec\n",
    "from hartufo.torch import collate_dict_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision.transforms import RandomRotation, CenterCrop, ToTensor\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('../hartufo-collections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CipicPlane(base_dir / 'CIPIC', side='any-left', plane='horizontal', hrir_role='target', other_specs=dict(\n",
    "        features_spec=ImageSpec(transform=[RandomRotation(10), ToTensor()]),\n",
    "        group_spec=SubjectSpec(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToPILImage()(ds[0]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photos are loaded as given, which means they could be different sizes. This leads to problems when trying to batch them together in a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds_loader = DataLoader(ds, batch_size=8, collate_fn=collate_dict_dataset)\n",
    "    next(iter(ds_loader))\n",
    "except RuntimeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to add a (PyTorch) Transform that outputs images of equal size, e.g. `RandomCrop`. But what is the size we want to crop to? Needs to be adjusted for to the original size. Let's find out then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = np.min([i['features'].shape[-2:] for i in ds])\n",
    "min_width, min_height = np.min([i['features'].shape[-2:] for i in ds], axis=0)\n",
    "min_size, min_width, min_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the minimum size of all images, we can add another Transform to the processing pipeline. Since it's a stochastic transformation, it makes sense to add it as a `transform`, not a `preprocessing`. This will help with overfitting. An additional advantage is that we don't need to load the dataset again, but can just append the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.append_transform(ImageSpec, CenterCrop(min_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now batching the data works, and the all image sizes are the equal to the one given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_loader = DataLoader(ds, batch_size=8, collate_fn=collate_dict_dataset)\n",
    "X, y = next(iter(ds_loader))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToPILImage()(ds[0]['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
